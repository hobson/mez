Zeke, Thunder, Joe and I are comparing the performance of several Hyperparameter Optimization (e.g. autotuning) algorithms for a presentation at the Wolfram Data Summit in September.


Advances in neural networks and deep learning have renewed interest in algorithms to assist in or fully automate tuning of the learning parameters for high-dimensional models with many "meta" or hyperparameters to be tuned. Open source libraries such as scikit-learn provide ready access to simple but inefficient algorithms such as exhaustive search and random search. Recently, statistical optimization approaches have proven to be better at tuning model parameters than humans and more efficient that exhaustive or random approaches, especially in high-dimensional domains such as image and speech processing.[1] Similarly a Sequential Model-Based Global Optimization (SMGO) meta-learning approach has improved efficiency further by using an approximation of the model training results as a heuristic within the tree search for optimal parameters.[2] In this paper we will demonstrate these automatic model parameter optimization algorithms on lower dimensional problems in order to compare them to the straight-forward approach in which each model parameter is optimized independently of the others using hill-climbing in one dimension at a time. Model performance metrics often form a convex surface when explored over the  space of possible model meta-parameters, especially for models with a large number of internal degrees of freedom, such as neural nets and Bayesian models and many of the meta-parameters are proportional to the number of these degrees of freedom. As a result, heuristics are often helpful in reducing the computational complexity of the meta-search and sequential single-dimension meta optimization can often produce results competitive with more thorough searches. This paper will compare results for this sequential single-dimension "Manhattan search" approach with the more complex and efficient SMGO approach for several toy and real-world problems.


[1] Snoek, Larachelle, Adams, "Practical Bayesian Optimization of Machine Learning Algorithms", 2014

[2] Bergstra, Bardenet, Bengio, and KÃÅegl, "Algorithms for Hyper-Parameter Optimization," 2014

